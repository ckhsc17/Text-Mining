{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_bert\n",
        "'''\n",
        "!wget -q\n",
        "!unzip\n",
        "'''\n",
        "\n",
        "from keras_bert import extract_embeddings\n",
        "from keras_bert import load_vocabulary\n",
        "from keras_bert import Tokenizer\n",
        "import os"
      ],
      "metadata": {
        "id": "DszWsvBC7b94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVrPgrEb7pfa"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "current_dir = '/content/drive/MyDrive/Colab Notebooks/'\n",
        "os.chdir(current_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#BERT-Tiny\n",
        "model_path = 'uncased_L-2_H-128_A-2'\n",
        "dict_path = 'uncased_L-2_H-128_A-2/vocab.txt'\n",
        "'''\n",
        "#BERT-base Chinese\n",
        "\n",
        "model_path = 'uncased_L-12_H-768_A-12'\n",
        "dict_path = 'uncased_L-12_H-768_A-12/vocab.txt'\n",
        "\n",
        "\n",
        "bert_token_dict = load_vocabulary(dict_path)\n",
        "bert_tokenizer = Tokenizer(bert_token_dict)"
      ],
      "metadata": {
        "id": "QBEJmFGy7_S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "inputs = tokenizer(input_text, return_tensors='pt')"
      ],
      "metadata": {
        "id": "ctw3s3YDQ8Ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reading in the text data and applying word embedding\n",
        "tokens = []\n",
        "embeddings = [] #like the TFIDF in previous examples but extracted by BERT method\n",
        "contents = []\n",
        "for i in range(1,100): #iterate all data\n",
        "\n",
        "    with open (\"./TrainingData/\"+str(i)+\".txt\",'r', encoding='utf-8') as data: #using the designated path\n",
        "        content = data.read()\n",
        "        data.close()\n",
        "        contents.append(content)\n",
        "        print(\"data\",i,\"read\")\n",
        "        '''\n",
        "        token = bert_tokenizer.tokenize(content)\n",
        "        tokens.append(token)\n",
        "        indices, segments = bert_tokenizer.encode(content)\n",
        "        print(len(token))\n",
        "        print(indices)\n",
        "        print(segments) #still need to understand its meaning\n",
        "        print(token)\n",
        "        print()\n",
        "        print(token[0])\n",
        "        print()\n",
        "\n",
        "        embedding = extract_embeddings(model_path, content)\n",
        "        embeddings.append(embedding)\n",
        "        print(len(embeddings))\n",
        "        print(len(embeddings[0]))\n",
        "        print(len(embeddings[0][0]))\n",
        "        print(embeddings[0][0]) #this is the CLS embedding of a single context\n",
        "        '''\n"
      ],
      "metadata": {
        "id": "tIqTrSlGFSDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D1_DdGvvLjie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting CLS as embedded word vectors\n",
        "embeddings = extract_embeddings(model_path, contents)\n",
        "print(len(embeddings)) #1096\n",
        "print(len(embeddings[0])) #tokenize length\n",
        "print(len(embeddings[0][0])) #model vector dimension\n",
        "print(embeddings[0][0])\n",
        "\n",
        "cls_embeddings = [token[0] for token in embeddings]"
      ],
      "metadata": {
        "id": "vY5OUCBSCbhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(cls_embeddings[0])\n",
        "print(embeddings[0][1])"
      ],
      "metadata": {
        "id": "NasJCokBDFnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting method 1: svm method - linear kernel\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "gVvPvQU2MfzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read in training_new.txt\n",
        "\n",
        "docs = [] #一開始先存資料庫裡doc的編號，之後再把他轉成存bert word embedding\n",
        "labels = []\n",
        "\n",
        "with open (\"training_new.txt\", 'r') as trainingData:\n",
        "  trainingData = trainingData.readlines()\n",
        "  print(\"trainingData: \", trainingData)\n",
        "  for data in trainingData:\n",
        "    print()\n",
        "    print(\"data: \", data)\n",
        "    data = data.split()\n",
        "\n",
        "    for i in range(len(data)-1):\n",
        "      labels.append(int(data[0]))\n",
        "    docs.extend(data[1:])\n",
        "\n",
        "\n",
        "print(docs)\n",
        "print(labels)\n",
        "print(len(labels))"
      ],
      "metadata": {
        "id": "41xLMBX-MqYV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}